import pysam
import os
import re
import threading
from multiprocessing import Pool, cpu_count
import pandas as pd
import numpy as np
import pickle
import itertools
import fastawork

def get_list_total_reads(directory_path, contig_tuple):
    '''Inputs: path to a given directory and a given contig.
    Function creates a list of .txt files that contain the filtering log generated by encode_filter_BAM.py. Iterates
    through each .txt file in the list, calculating for the number of reads remaining after filtering for the given
    contig. Stores these numbers in the global variable num_reads_dict in the format
    {filename: #_of_reads_after_filtering}
    '''
    global contig_num_reads_dict
    num_reads_dict = {}

    contig, contig_length = contig_tuple

    #Generates list of bam.txt files in the given directory.
    file_list = os.listdir(directory_path)
    if not directory_path.endswith('/'):
        directory_path += '/'
    file_list = list(filter(lambda filename: filename.endswith("bam.txt"), file_list))

    #Iterates through each file in the list of files in the directory.
    #For each .txt file, uses regex to calculate the number of reads remaining after filtering
    #Stores the filename and its calculated value in the global variable num_reads_dict
    for filename in file_list:
        finished = False
        with open(directory_path+filename, "r") as file:
            for line in file.readlines():
                regex_pattern = re.escape(contig) + ": (\d*) of (\d*) "
                match = re.search(regex_pattern, line) #Contig 25s: 3634 of 2979650 (0.121960632960247%) filtered.
                if match:
                    num_reads_dict[filename.split(".")[0]] = int(match.group(2)) - int(match.group(1))
                    finished = True
                    continue
        if not finished:
            print(filename, "did not finish.")
    contig_num_reads_dict[contig] = num_reads_dict

def get_pileup(file_path, contig_tuple):
    global contig_num_reads_dict

    contig, contig_length = contig_tuple

    #Opens .bam file and pulls out num_reads_dict for the given contig
    samfile = pysam.AlignmentFile(file_path, "rb")
    num_reads_dict = contig_num_reads_dict[contig]

    #looks for the sample name in the filename (assumes .bam filename is formatted: samplename_filtered.bam) and only keeps samplename
    sample_name = re.search("(ENCSR[0-9A-Za-z_]*)_filtered.bam$", file_path).group(1)

    #Not sure what this does entirely?
    pos_depth_list = pd.DataFrame([*samfile.count_coverage(contig, quality_threshold=0, read_callback="nofilter")]).sum().T

    if contig == "18s":
        print(sample_name)
        print('----------')
        for i in range(109, 115):
            print(i, contig, pos_depth_list[i], num_reads_dict[sample_name], pos_depth_list[i]/num_reads_dict[sample_name]* 10 ** 6)
    if len(pos_depth_list) != 0:
        pos_depth_list = pos_depth_list / num_reads_dict[sample_name] * 10 ** 6
    else:
        pos_depth_list = pd.Series()
    # print(pos_depth_list)
    pos_depth_list.name = sample_name
    return (sample_name, contig, pos_depth_list)




#Defining some global variables used by the functions
#contig_num_reads_dict = {contig: num_reads_dict}
#num_reads_dict = {filename: #_of_reads_after_filtering}
contig_num_reads_dict = {}

#Contigs to scan through
# contig_list = ["18s","28s","5.8s","5s"]
contig_list = fastawork.extract_fasta_info("/Users/lij/PycharmProjects/eclip_analysis_pipeline/hsrRNA.fa").items()

path = "/Users/lij/PycharmProjects/eclip_analysis_pipeline/pileup_test_data"
if not path.endswith('/'):
    path = path + '/'


#Iterate through each contig in contig_list to generate num_reads_dict for each contig stored in contig_num_reads_dict
for contig in contig_list:
    get_list_total_reads(path, contig)

#Get all files in the directory path and generates list of files.
file_list = os.listdir(path)
file_list = list(filter(lambda filename: filename.endswith("_filtered.bam"), file_list))
path_file_list = [path+filename for filename in file_list]

#Generate argument tuples to pass into p.starmap()
#Ex. (filename1, contig1), (filename2, contig1), (filename1, contig2) etc.
starmap_file_list = itertools.product(path_file_list, contig_list)

p = Pool(cpu_count()-1)
try:
    map_list = p.starmap(get_pileup, starmap_file_list)
except:
    p.terminate()
    p.close()
    raise

pileup_data = {}

for output in map_list:
    contig = output[1]
    if not contig in pileup_data.keys():
        pileup_data[contig] = output[2]
    else:
        pileup_data[contig] = pd.concat([pileup_data[contig],output[2]], axis=1)

pileup_output_path = path +"pileup_output/"
if not os.path.exists(pileup_output_path):
    os.mkdir(pileup_output_path)

for contig, contig_length in contig_list:
    pileup_contig_data = pileup_data[contig]
    pileup_contig_data.fillna(0, inplace=True)
    output_name = pileup_output_path + contig + "_pileup_table_FIXED" + ".csv"
    pileup_contig_data.to_csv(output_name)

# map_unzip = list(zip(*map_list))
# pileup_data = pd.concat(map_unzip[1], axis=1)
# pileup_data.columns = map_unzip[0]
# pileup_data.fillna(0, inplace=True)



'''ANTHONY'S IPYNB CODE

# get list of total reads
path = "./logs/alignment/" #"./aligned_reads/BAMs/temp_files/filtering/"
file_list = os.listdir(path)
#file_list = list(filter(lambda filename: filename.endswith("bam.txt"), file_list))
file_list = list(filter(lambda filename: filename.endswith("stderr.txt"), file_list))
#assert len(file_list) == 98

num_reads_dict = {}
for filename in file_list:
    finished = False
    with open(path+filename, "r") as file:
        for line in file.readlines():
            match = re.search("^(\d*) reads; of these:", line) #3634 of 2979650 (0.121960632960247%) filtered.
            #match = re.search("^(\d*) of (\d*) \([0-9.%]*\) filtered.", line)
            if match:
                num_reads_dict[filename.split(".")[0]] = int(match.group(1))
                #num_reads_dict[filename.split("_")[0]] = int(match.group(2)) - int(match.group(1))
                finished = True
                continue
    if not finished:
        print(filename, "did not finish.")

print("Done")


def get_pileup(file_path):
    global num_reads_dict
    samfile = pysam.AlignmentFile(file_path, "rb")

    sample_name = re.search("(ENCSR[0-9A-Z])_", file_path).group(1)
    pos_depth_list = pd.DataFrame([*samfile.count_coverage("NR_003278.3", 0, 1870, quality_threshold=0)]).sum().T
    if len(pos_depth_list) != 0:
        pos_depth_list = pos_depth_list / num_reads_dict[sample_name] * 10 ** 6
    else:
        pos_depth_list = pd.Series()
    return (sample_name, pos_depth_list)

path = "./aligned_reads/BAMs/filtered/"
file_list = os.listdir(path)
file_list = list(filter(lambda filename: filename.endswith(".bam"), file_list))
path_file_list = [path+filename for filename in file_list]
assert len(file_list) == 98

p = Pool(90)
try:
    map_list = p.map(get_pileup, path_file_list)
except:
    p.terminate()
    p.close()
    raise

map_unzip = list(zip(*map_list))
pileup_data = pd.concat(map_unzip[1], axis=1)
pileup_data.columns = map_unzip[0]
pileup_data.fillna(0, inplace=True)

'''